{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* __Student name:__ Cassarra Groesbeck\n",
    "* __Student pace:__ Part Time/ Flex\n",
    "* __Scheduled project review date/time:__ \n",
    "* __Instructor name:__ \n",
    "* __Blog post URL:__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction \n",
    "- Every year, more than 795,000 people in the United States have a stroke. About 610,000 of these are first or new strokes. About 87% of all strokes are ischemic strokes, in which blood flow to the brain is blocked.[Apr 5, 2022, CDC.gov](https://www.cdc.gov/stroke/facts.htm)\n",
    "\n",
    "- From 1990 to 2019, the change in the prevalence of stroke in the general population increased by about 60%. [Feb 3, 2022, newsroom.heart.org](https://newsroom.heart.org/news/u-s-stroke-rate-declining-in-adults-75-and-older-yet-rising-in-adults-49-and-younger)\n",
    "\n",
    "- Strokes are the No. 5 cause of death and a leading cause of disability in the United States. 80% of strokes are preventable.[American Stoke Association](https://www.stroke.org/en/about-stroke)\n",
    "\n",
    "\n",
    "\n",
    "## 1a. Objectives\n",
    "Because of the nature of this problem, and in order to capture as many potential stroke victims as possible, I feel it appropriate to be very aggressive with stroke predictions. For this reason I have chosen Recall as the evaluation metric for my models. This will inevitably lead to extra false positives, however, because of the health risks associated with strokes, I feel the measures needed to declare an individual to NOT be at risk, outweigh a misclassification of at risk.\n",
    "\n",
    "\n",
    "## 1b. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding\n",
    "This dataset contains 5110 observations with 12 attributes (11 clinical features) for predicting stroke events. \n",
    "\n",
    "\n",
    "### 2a. Attribute Information\n",
    "| Column     | Description   |\n",
    "|------------|:--------------|\n",
    "| `id`               | **unique identifier**  |\n",
    "| `gender`           | **\"Male\", \"Female\" or \"Other\"**  |\n",
    "| `age`              | **age of the patient** |\n",
    "| `hypertension`     | **0 if the patient doesn't have hypertension, 1 if the patient has hypertension**  |\n",
    "| `heart_disease`    | **0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease**   |\n",
    "| `ever_married`     | **\"No\" or \"Yes\"**  |\n",
    "| `work_type`        | **\"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"**   |\n",
    "| `Residence_type`   | **\"Rural\" or \"Urban\"**  |\n",
    "| `avg_glucose_level`| **average glucose level in blood**  |\n",
    "| `bmi`              | **body mass index** |\n",
    "| `smoking_status`   | **\"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"***  |\n",
    "| `stroke`           | **1 if the patient had a stroke or 0 if not**  |\n",
    "|    **_*Note:_**      | _\"Unknown\" in_ `smoking_status` _means that the information is unavailable for this patient_ |\n",
    "\n",
    "\n",
    "### 2b. Acknowledgements\n",
    "Data comes from the [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) and can be found on [kaggle](https://www.kaggle.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#plt.style.use('seaborn')\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9', \"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "%matplotlib inline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix,\\\n",
    "    precision_score, recall_score, accuracy_score, f1_score, log_loss,\\\n",
    "    roc_curve, roc_auc_score, classification_report, plot_roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploring the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Load and visually check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/healthcare-dataset-stroke-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check of raw df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Drop unnecessary column, and identify target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'id' column\n",
    "df = df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Target Feature\n",
    "target = 'stroke'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. The basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Exploring nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of missing data\n",
    "f'{(201/5110)*100:.3}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just the nulls\n",
    "bmi_nulls = df[df['bmi'].isnull()]\n",
    "# `stroke` patients with missing `bmi` data\n",
    "bmi_nulls['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4e. Distribution of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Distribution of Target Feature')\n",
    "print('-'*30)\n",
    "print('COUNTS:')\n",
    "print(df[target].value_counts())\n",
    "print('- '*15)\n",
    "print('PERCENTAGES:')\n",
    "for value in [norm_value_count for norm_value_count \\\n",
    "                in enumerate(df[target].value_counts(normalize=True))]:\n",
    "    print(f'{value[0]}\\t{value[1]*100:.4}%')\n",
    "print(f'Name: {target}, dtype: int64') # just for symmetry\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = df.drop(target, axis=1)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(f'Distribution of Other Features')\n",
    "print(\"-\"*30)\n",
    "for column in show.columns:\n",
    "    print(\"-\"*30)\n",
    "    print(f\"UNIQUE VALUES: {len(show[column].unique())}\")\n",
    "    if len(show[column].unique()) <= 5:\n",
    "        print(\"- \"*15)\n",
    "        print(show[column].value_counts())\n",
    "    else:\n",
    "        print(\"- \"*15)\n",
    "        print(f'\\t\\t  MIN: {show[column].min()}')\n",
    "        print(f'\\t\\t  MEAN: {round(show[column].mean())}')\n",
    "        print(f'\\t\\t  MAX: {show[column].max()}')\n",
    "        print((f'Name: {column}, dtype: float64')) \n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4f. Visualization of distribution of values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "fig, axes = plt.subplots(ncols=4, nrows=3, figsize=(12, 10))\n",
    "fig.set_tight_layout(True)\n",
    "# plot\n",
    "for index, col in enumerate(df.columns):\n",
    "    ax = axes[index//4][index%4]\n",
    "    sns.histplot(data=df[col], ax=ax, linewidth=0.1, alpha=1)\n",
    "    ax.tick_params(axis='x', rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4g. Findings\n",
    "\n",
    "- `gender`\n",
    " - There is only 1 'other' value, for simplicity I will drop this 1 row.\n",
    " - About 1000 more women than men in this dataset.\n",
    "- `age`\n",
    " - The youngest patient is under 1 yo. Oldest patient is 82.\n",
    " - Decent distribution, \n",
    " - Average age of patients in this dataset are 43 years old. \n",
    "- `hypertension`\n",
    " - Binary, 1 if the patient has hypertension.\n",
    " - Very similar distribution as target feature.\n",
    "- `heart_disease`\n",
    " - Binary, 1 if the patient has heart disease.\n",
    " - Almost identical distribution as target feature.\n",
    " - I am curious how `hypertension` & `heart_disease` will correlate with eachother. \n",
    "- `ever_married`\n",
    " - About 65/35 split with majority of patients listed as 'Yes.'\n",
    "- `work_type`\n",
    " - Only 5 categories. Not surprisingly, more than half answered 'Private.'\n",
    " - Other 4 make up little more than 40%.\n",
    " - 'Never_worked' is less than 1%.\n",
    "- `Residence_type`\n",
    " - Almost 50/50 split between 'Urban' and 'Rural.'\n",
    "- `avg_glucose_level` & `bmi` \n",
    " - continuous features that are both skewed positively.\n",
    " - `bmi` is missing 201 values (3.93%). Of those missing values, 40 are stroke patients.\n",
    "   - because of this missing data an imputer is required for modeling\n",
    "- `smoking_status`\n",
    " - about 30% of patients are listed as 'Unknown.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4h. As indicated above (4g): Drop 1 'other' value in `gender`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(df[df['gender']=='Other'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Seperate and Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Separate data into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(target, axis = 1)\n",
    "y = data[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    stratify=y, \n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Which features are better predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how search relates to business problem, why am I looking where I am looking, What am I trying to find?\n",
    "start with pairplot, pps, hoping to find correlation in easy to identify features such as age, gender (errrr, it's \"sex\" but ok), bmi, residence type, and smoking. Explain this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Make a visualization data frame with `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Pairplot\n",
    "Why a pairplot, what will this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c. PPS \n",
    "what is a pps, link to article, what will it do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d. Explore top 3 individual features relative to stroke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Very important words about my function\n",
    "This function does all the heavy lifting for me. It allows me to bypass grid search, for establishing baseline models and for testing a best parameters model on test data without needing to repeat the computationally expensive grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_check_model(model,\n",
    "                     X_train=X_train,\n",
    "                     X_test=X_test,\n",
    "                     y_train=y_train,\n",
    "                     y_test=y_test,\n",
    "                     test_size=.20, \n",
    "                     random_state=seed, \n",
    "                     imputer=None,\n",
    "                     smote=None,             \n",
    "                     scaler=None,\n",
    "                     grid_search=True,\n",
    "                     param_dict=None,\n",
    "                     use_test_data=False, \n",
    "                     show_classification_report=False, \n",
    "                     show_thresholds_table=False,  \n",
    "                     show_plots=False,\n",
    "                     display_labels=None):\n",
    "    \n",
    "    \"\"\"   \n",
    "    Uses sklearn.model_selection.train_test_split to divide data into train and test sets.\n",
    "    \n",
    "    If data has NaN values, an sklearn imputer to must be specified. \n",
    "    \n",
    "    Option to use any sklearn scaler to scale data. \n",
    "    Option to use any SMOTE (must be appropiate for data, ie, categorical vs continuous, or both)\n",
    "    Option to use sklearn GridSearchCV to search for best paramaeters for model.  \n",
    "    Option to specify use of Test data for evaluation metrics.\n",
    "    \n",
    "    \n",
    "    Output\n",
    "    ---------- \n",
    "    (optional) Classification Report\n",
    "    (optional) Thresholds, FPRs, TPRs Stats Table with AUC score\n",
    "    (optional) Plots Confusion matrix and if available an ROC curve\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Recall score\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    " \n",
    "    model : supervised learning model to be evaluated. \n",
    "    \n",
    "    pipe_grid_param_dict : dict or list of dictionaries\n",
    "        sklearn.model_selection.GridSearchCV parameter: \n",
    "            Dictionary with parameters names (`str`) as keys and lists of\n",
    "            parameter settings to try as values, or a list of such\n",
    "            dictionaries, in which case the grids spanned by each dictionary\n",
    "            in the list are explored. This enables searching over any sequence\n",
    "            of parameter settings.\n",
    "    \n",
    "    data : pandas data frame, default=data\n",
    "    \n",
    "    target : string or variable set to a string, default=target\n",
    "    \n",
    "    test_size : float or int, default=.20\n",
    "        sklearn.model_selection.train_test_split parameter:\n",
    "            If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "            of the dataset to include in the test split. If int, represents the\n",
    "            absolute number of test samples.\n",
    "    \n",
    "    random_state : int or RandomState instance, default=42\n",
    "        Controls the shuffling applied to the data before applying the split.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "\n",
    "    use_test_data : boolean True or False, default=False\n",
    "        determines the data used to asses model performance\n",
    "    \n",
    "    show_thresholds_report : boolean True or False, default=False\n",
    "        Print table with AUC score, Thresholds, FPR's and TPR's\n",
    "    \n",
    "    show_plots : boolean True or False, default=False\n",
    "        If target variable is binary, plots Confusion Matrix and ROC curve.\n",
    "        Otherwise just Confusion Matrix\n",
    "    \n",
    "    display_labels : list or 'None', default=None\n",
    "        If the target is binary 0,1 the labels can be changed to more descriptive labels. \n",
    "        example: ['Healthy', 'HeartDisease']\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ######################################################################\n",
    "    # 1. TRANSFORM-DATA PIPELINE                                         #\n",
    "    ######################################################################   \n",
    "    \n",
    "    \n",
    "    # 1a. Seperate by type of data\n",
    "    X_train_nums = X_train.select_dtypes('float64')\n",
    "    X_train_cat = X_train.select_dtypes('object')\n",
    "\n",
    "\n",
    "    # 1b. Pipeline 1 (numerical data)\n",
    "    numerical_pipeline = Pipeline(steps=[\n",
    "        ('scaler', scaler)])\n",
    "    \n",
    "    \n",
    "    # 1c. Pipeline 2 (categorical data)  \n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "    # 1d. Converge pipelines 1 & 2\n",
    "    trans = ColumnTransformer(transformers=[\n",
    "        ('numerical', numerical_pipeline, X_train_nums.columns),\n",
    "        ('categorical', categorical_pipeline, X_train_cat.columns)],\n",
    "                             remainder='passthrough')\n",
    "        \n",
    "    \n",
    "    # 1e. Model with converged pipeline\n",
    "    model_pipe = imbpipeline(steps=[\n",
    "        ('trans', trans),\n",
    "        ('imputer', imputer),\n",
    "        ('smote', smote),\n",
    "        ('model', model)])\n",
    "\n",
    "    \n",
    "    ######################################################################\n",
    "    # 2. GRID SEARCH PIPELINE                                            #\n",
    "    ######################################################################   \n",
    "    \n",
    "\n",
    "    # 2a. Determine if using gridsearch\n",
    "    if grid_search:\n",
    "        best_model = GridSearchCV(estimator=model_pipe, \n",
    "                           param_grid=param_dict, \n",
    "                           scoring='recall', \n",
    "                           cv=3)\n",
    "\n",
    "        \n",
    "    # fit the model to evaluate\n",
    "        fit_model = best_model.fit(X_train, y_train) \n",
    "    \n",
    "    \n",
    "    else: \n",
    "        fit_model = model_pipe.fit(X_train, y_train) \n",
    "   \n",
    "\n",
    "    ######################################################################\n",
    "    # 3. PRINT MODEL DETAILS                                             #\n",
    "    ######################################################################\n",
    "\n",
    "    \n",
    "    # 3a. Determine if using GridSearch\n",
    "    if grid_search:\n",
    "        \n",
    "        \"\"\"\n",
    "        # 3a-1. Use .best_params_ dict and reformat to print the same way \n",
    "            classifiers/scalers/imputers etc are instantiated.\n",
    "\n",
    "\n",
    "            Example: \n",
    "            ----------\n",
    "\n",
    "               .best_params_ = {'model__criterion': 'gini', \n",
    "                                'model__max_depth': 6, \n",
    "                                'scaler__with_mean': True} \n",
    "\n",
    "                prints as: \n",
    "                    DecisionTreeClassifier(criterion='gini', max_depth=6, ..., N-param=N-value) \n",
    "                    StandardScaler(with_mean=True)\n",
    "        \"\"\"  \n",
    "        kind_of_params = {}\n",
    "        for k,v in fit_model.best_params_.items():\n",
    "            key = k.split(\"__\")[0]\n",
    "            if key not in kind_of_params.keys():\n",
    "                kind_of_params[key] = \"\" \n",
    "            if k.split(\"__\")[1] == 'solver':\n",
    "                kind_of_params[key] += k.split(\"__\")[1]+\"='\"+str(v)+\"', \" # solver has qoutes around it\n",
    "            elif k.split(\"__\")[1] == 'criterion':\n",
    "                kind_of_params[key] += k.split(\"__\")[1]+\"='\"+str(v)+\"', \" # criterion has qoutes around it\n",
    "            else: \n",
    "                kind_of_params[key] += k.split(\"__\")[1]+\"=\"+str(v)+\", \" #<-- notice comma\n",
    "\n",
    "\n",
    "        # 3a-2. Remove extra comma at end of each dic value\n",
    "        for k, v in kind_of_params.items():\n",
    "            kind_of_params[k] = v[:-2]\n",
    "\n",
    "\n",
    "        # 3a-3. Print in copy paste format:\n",
    "        # Ex. DecisionTreeClassifier(criterion='gini', ..., paramN=value)\n",
    "        if 'model' in  kind_of_params.keys():\n",
    "            model_text = str(model).split(\")\")[0]+\", \"+kind_of_params['model']+\")\"\n",
    "            print(model_text) \n",
    "        else:\n",
    "            print(model) # otherwise, use what was fed into function\n",
    "\n",
    "   \n",
    "        if 'imputer' in kind_of_params.keys():\n",
    "            imputer_text = str(imputer).split(\"()\")[0]+\"(\"+kind_of_params['imputer']+\")\"\n",
    "            print(imputer_text)\n",
    "        else:\n",
    "            print(imputer)\n",
    "\n",
    "\n",
    "        if scaler != None:\n",
    "            if 'scaler' in kind_of_params.keys():\n",
    "                scaler_text = str(scaler).split(\"()\")[0]+\"(\"+kind_of_params['scaler']+\")\"\n",
    "                print(scaler_text) \n",
    "            else:\n",
    "                print(scaler)  \n",
    "\n",
    "\n",
    "        if smote != None:\n",
    "            if 'smote' in kind_of_params.keys():\n",
    "                smote_text = str(smote).split(\")\")[0]+\", \"+kind_of_params['smote']+\")\"\n",
    "                print(smote_text)\n",
    "            else:\n",
    "                print(smote)\n",
    "\n",
    "    # 3b. If bypassing GridSearch             \n",
    "    else: \n",
    "        \n",
    "        # 3b-1 Print what was fed into function, example: DecisionTreeClassifier()\n",
    "        # model & imputer are manditory\n",
    "        print(model)  \n",
    "        print(imputer)\n",
    "\n",
    "        # scaler and/or smote are optional (will not print if not used) \n",
    "        if scaler != None:  \n",
    "            print(scaler)\n",
    "        if smote != None:\n",
    "            print(smote)\n",
    "            \n",
    "    # 3c. Print type of data used in model evaluation\n",
    "    if use_test_data:\n",
    "        data_used_text = \"Test\"\n",
    "    else:\n",
    "        data_used_text = \"Training\"\n",
    "    \n",
    "    print(f'\\nModel evaluated with {data_used_text} data\\n')\n",
    "    \n",
    "\n",
    "    ######################################################################\n",
    "    # 4. (Optional) CLASSIFICATION REPORT                                #\n",
    "    ######################################################################\n",
    "\n",
    "    \n",
    "    # 4a. Assign variables based on data using for evaluation\n",
    "    if use_test_data:\n",
    "        X_true = X_test\n",
    "        y_true = y_test\n",
    "    else: \n",
    "        X_true = X_train\n",
    "        y_true = y_train\n",
    "        \n",
    "        \n",
    "    # 4b. Make predictions and print report \n",
    "    y_preds = fit_model.predict(X_true)\n",
    "    cr = classification_report(y_true, y_preds, digits=4)\n",
    "    \n",
    "    print('-'*54)\n",
    "    print('\\t\\tCLASSIFICATION REPORT')\n",
    "    print('-'*54)\n",
    "    print(cr)\n",
    "    print('-'*54)\n",
    "        \n",
    "\n",
    "        \n",
    "    ######################################################################\n",
    "    # 5. (Optional) THRESHOLDS TABLE                                     #\n",
    "    ######################################################################   \n",
    "   \n",
    "\n",
    "    if show_thresholds_table:\n",
    "        # 5a. Calculate the probability scores\n",
    "        if ('LogisticReg' in str(model)):\n",
    "            y_score = fit_model.decision_function(X_true) \n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "        else:\n",
    "            y_score = fit_model.predict_proba(X_true)\n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_score[:,1]) # <-- probability of Class 1\n",
    "\n",
    "        # 5b. Format values and print\n",
    "        # To display as: THRESHOLD: value | FPR: percent%, TPR:percent%\n",
    "        thresh_fp_tp = list(zip(thresholds, fpr, tpr))\n",
    "        these_to_print = [f'THRESHOLD: {e[0]:.2f} | FPR: {e[1]:.2%}, TPR:{e[2]:.2%}' \\\n",
    "                          for e in thresh_fp_tp]        \n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "        print('-'*54)\n",
    "        print('\\t\\t  THRESHOLD STATS')\n",
    "        print('-'*54)\n",
    "        print(f'AUC: {auc_score}')\n",
    "        print('- '*23)\n",
    "        for element in these_to_print:\n",
    "            print(element)\n",
    "        print('-'*54)\n",
    "        \n",
    "\n",
    "    ######################################################################\n",
    "    # 6. (Optional) VISUALIZATIONS                                       #\n",
    "    ######################################################################\n",
    "    \n",
    "    \n",
    "    if show_plots: \n",
    "        # Figure set up\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "        fig.suptitle(f'Model Evaluated on {data_used_text} Data', color='tab:blue', size=14)\n",
    "        \n",
    "\n",
    "        # Left  \n",
    "        axes[0].set_title(\"Confusion Matrix\", size=15)  \n",
    "        axes[0].grid(False)\n",
    "        plot_confusion_matrix(fit_model, X_true, y_true, \n",
    "                              cmap=plt.cm.Blues, \n",
    "                              ax=axes[0], \n",
    "                              display_labels=display_labels,\n",
    "                              normalize='true')\n",
    "\n",
    "        # Right \n",
    "        axes[1].set_title(\"ROC Curve\", size=15)\n",
    "        plot_roc_curve(fit_model, X_true, y_true, ax=axes[1]);\n",
    "\n",
    "\n",
    "print(\"ran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_column_indices = [X_train.columns.get_loc(c) \\\n",
    "                      for c in X_train.columns \\\n",
    "                      if c in X_train.select_dtypes('object').columns]\n",
    "\n",
    "p_dic = {'model__criterion': ['gini', 'entropy'],\n",
    " 'model__max_depth': [1, 2],\n",
    " 'model__min_samples_split': [2,10],\n",
    " 'model__min_samples_leaf': [1, 6]}\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=seed)\n",
    "\n",
    "imputer=IterativeImputer(random_state=seed)\n",
    "smote=SMOTENC(categorical_features=cat_column_indices, random_state=seed)             \n",
    "scaler=StandardScaler()\n",
    "#grid_search=True\n",
    "#param_dict=None,\n",
    "#use_test_data=False, \n",
    "#show_classification_report=False, \n",
    "#show_thresholds_table=False,  \n",
    "#show_plots=False,\n",
    "#display_labels=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_model(model, imputer=imputer, grid_search=False, use_test_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_dic={0: round(data[target].value_counts(normalize=True)[1],6), \\\n",
    "              1: round(data[target].value_counts(normalize=True)[0],6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LogisticRegression(random_state=seed, class_weight=class_weight_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_model(model3, imputer=imputer, grid_search=False, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_model(model3, imputer=imputer, smote=smote, grid_search=False, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_model(model3, imputer=imputer, smote=smote, grid_search=False, show_plots=True, use_test_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- for loops for baseline models imputer with different imputers, \n",
    "- add return to function, I want scores \n",
    "- do more visualization, find corrs to start models small, one or two features then add features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Set up for modeling\n",
    "have cat_cols, instantiate baseline models, imputers, and scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Modeling\n",
    "Explain choices at each step, what did you find. why are you moving in direction you are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Final Model\n",
    "why this final model? What is so great about it. how does it solve business problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
